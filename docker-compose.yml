version: "3.4"

x-testing-build: &testing-build
  build:
    context: .
    dockerfile: Dockerfile
    args:
      target: .[testing]

x-networks: &networks
  networks:
    - ns

services:
  consul:
    <<: *networks
    image: consul:1.6.2
    ports:
      - 8500:8500

  legacy-database:
    <<: *networks
    image: mysql:5.7
    ports:
      - 3306:3306
    environment:
      - MYSQL_HOST=legacy-database
      - MYSQL_USER=lumenghe
      - MYSQL_PASSWORD=lumenghe
      - MYSQL_ROOT_PASSWORD=lumenghe
      - MYSQL_DATABASE=lumenghe
    volumes:
      - ./.docker/lumenghe-mysql/mysql.cnf:/etc/mysql/conf.d/mysql.cnf
      - ./.docker/lumenghe-mysql/fixtures/legacy-database:/docker-entrypoint-initdb.d/

  zookeeper:
    <<: *networks
    image: confluentinc/cp-zookeeper:5.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    <<: *networks
    image: confluentinc/cp-enterprise-kafka:5.5.0
    ports:
      - '9092:9092'
      - '29092:29092'
      - '1111:1111'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_JMX_PORT: 1111
      KAFKA_JMX_HOSTNAME: 0.0.0.0
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: "true"
      CONFLUENT_SUPPORT_CUSTOMER_ID: "anonymous"
    depends_on:
      - zookeeper

  schema-registry:
    <<: *networks
    image: confluentinc/cp-schema-registry:5.5.0
    depends_on:
      - zookeeper
      - kafka
    ports:
      - '8081:8081'
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181

  ksqldb-server:
    <<: *networks
    image: confluentinc/ksqldb-server:0.9.0
    depends_on:
      - kafka
      - schema-registry
    ports:
      - "8088:8088"
    environment:
      KSQL_BOOTSTRAP_SERVERS: kafka:29092
      KSQL_LISTENERS: http://0.0.0.0:8088
      KSQL_KSQL_EXTENSION_DIR: /data/udf-jars/
      KSQL_KSQL_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"
    volumes:
      - ./ksqldb-udf/target/ksql-udf-functions-1.0.jar:/data/udf-jars/ksql-udf-functions-1.0.jar


  ksqldb-cli:
    <<: *networks
    image: confluentinc/ksqldb-cli:0.9.0
    depends_on:
      - ksqldb-server
      - kafka
    entrypoint: /bin/sh
    tty: true

  kafka-connect:
    <<: *networks
    build: .docker/lumenghe-kafka-connect
    ports:
      - 8083:8083
      - 2222:2222
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
      CONNECT_REST_PORT: 8083
      CONNECT_REST_LISTENERS: http://0.0.0.0:8083
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components,/local-connect-jars"
      KAFKA_JMX_PORT: 2222
      KAFKA_JMX_HOSTNAME: 0.0.0.0
    command:
      - /bin/bash
      - -c
      - |
        # Now launch Kafka Connect
        /etc/confluent/docker/run &
        #
        echo "Waiting for Kafka Connect to start listening on $$CONNECT_REST_ADVERTISED_HOST_NAME ⏳"
        while [ $$(curl -s -o /dev/null -w %{http_code} http://$$CONNECT_REST_ADVERTISED_HOST_NAME:$$CONNECT_REST_PORT/connectors) -ne 200 ] ; do
          echo -e $$(date) " Kafka Connect listener HTTP state: " $$(curl -s -o /dev/null -w %{http_code} http://$$CONNECT_REST_ADVERTISED_HOST_NAME:$$CONNECT_REST_PORT/connectors) " (waiting for 200)"
          sleep 5
        done

        # Don't let the container die
        sleep infinity
    #volumes:
      #- ./lumenghe-kafka-smt/target/:/local-connect-jars/kafka-connect-smt
    depends_on:
      - zookeeper
      - kafka
      - legacy-database

  kafka-hq:
    <<: *networks
    image: tchiotludo/kafkahq:0.11.0
    ports:
      - 8080:8080
    volumes:
      - ./.docker/kafka-hq/application.yml:/app/application.yml
    depends_on:
      - kafka
      - kafka-connect

  elasticsearch:
    <<: *networks
    image: 'docker.elastic.co/elasticsearch/elasticsearch:6.7.2'
    ports:
      - '9200:9200'
      - '9300:9300'
    environment:
      - bootstrap.memory_lock=true
      - discovery.type=single-node
      #- "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - cluster.routing.allocation.disk.threshold_enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    user: 'elasticsearch'

  kibana:
    <<: *networks
    depends_on:
      - elasticsearch
    image: docker.elastic.co/kibana/kibana:6.7.2
    ports:
      - '5601:5601'

  compile-kafka-smt:
    <<: *networks
    image: maven:3.5.4-jdk-10
    working_dir: /kafka-connect-smt
    command: mvn clean package
    volumes:
      - ./lumenghe-kafka-smt:/kafka-connect-smt

  compile-ksqldb-udf:
    <<: *networks
    image: maven:3.5.4-jdk-10
    working_dir: /ksqldb-udf
    command: mvn clean package
    volumes:
      - ./ksqldb-udf:/ksqldb-udf

  import-legacy-fixtures:
    <<: *networks
    image: google/cloud-sdk:latest
    command:
      - /bin/bash
      - -c
      - |
        gcloud auth activate-service-account --key-file=/certs/gcloud.json
        gsutil cp gs://xxxxxx-fixtures/struct_xxxx.sql.gz /fixtures/legacy/03-struct.sql.gz
        gsutil cp gs://xxxxxx-fixtures/partial_dump_xxxx.sql.gz /fixtures/legacy/04-data.sql.gz
    volumes:
      - ./gcloud.json:/certs/gcloud.json
      - ./.docker/lumenghe-mysql/fixtures/legacy-database:/fixtures/legacy

  kafka-stream-processor:
    <<: *networks
    build: ./kafka-stream-processor
    environment:
      - stream_processor__consul__host=consul
      - stream_processor__consul__port=8500

networks:
  ns:
    driver: bridge
